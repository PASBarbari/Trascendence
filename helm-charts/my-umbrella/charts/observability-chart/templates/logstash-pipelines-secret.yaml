apiVersion: v1
kind: Secret
metadata:
  name: logstash-pipelines-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "observability-chart.labels" . | nindent 4 }}
    app.kubernetes.io/component: logstash
type: Opaque
stringData:
  pipelines.yml: |
    - pipeline.id: beats-input
      path.config: "/usr/share/logstash/pipeline/beats.conf"
      pipeline.workers: 1
      pipeline.batch.size: 125
      pipeline.batch.delay: 50
      
    - pipeline.id: django-logs
      path.config: "/usr/share/logstash/pipeline/django.conf"  
      pipeline.workers: 1
      pipeline.batch.size: 50
      pipeline.batch.delay: 20
      
  beats.conf: |
    input {
      beats {
        port => 5044
        host => "0.0.0.0"
      }
    }
    
    filter {
      # Add Kubernetes metadata if available
      if [kubernetes] {
        mutate {
          add_field => { 
            "service_name" => "%{[kubernetes][container][name]}"
            "namespace" => "%{[kubernetes][namespace]}"
            "pod_name" => "%{[kubernetes][pod][name]}"
          }
        }
      }
      
      # Handle container logs
      if [fields][log_type] == "container" {
        grok {
          match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:log_message}" }
          tag_on_failure => ["_grokparsefailure_container"]
        }
      }
      
      # Handle nginx logs  
      if [fields][log_type] == "nginx" {
        grok {
          match => { "message" => "%{NGINXACCESS}" }
          tag_on_failure => ["_grokparsefailure_nginx"]
        }
      }
      
      # Parse JSON logs from Django (your apps already output JSON)
      if [message] =~ /^\{.*\}$/ {
        json {
          source => "message"
          target => "django"
          tag_on_failure => ["_jsonparsefailure"]
        }
        
        # Extract Django fields to top level
        if [django] {
          mutate {
            add_field => {
              "log_level" => "%{[django][level]}"
              "log_module" => "%{[django][module]}"
              "log_message" => "%{[django][message]}"
              "log_path" => "%{[django][path]}"
              "log_lineno" => "%{[django][lineno]}"
            }
            add_tag => ["django_structured"]
          }
          
          # Convert timestamp if present
          if [django][time] {
            date {
              match => [ "[django][time]", "yyyy-MM-dd HH:mm:ss,SSS", "ISO8601" ]
              target => "@timestamp"
            }
          }
        }
      }
      
      # Add service identification
      if [kubernetes][container][name] {
        if [kubernetes][container][name] =~ /chat/ {
          mutate { add_field => { "service_type" => "chat" } }
        } else if [kubernetes][container][name] =~ /login/ {
          mutate { add_field => { "service_type" => "login" } }
        } else if [kubernetes][container][name] =~ /user/ {
          mutate { add_field => { "service_type" => "user" } }
        } else if [kubernetes][container][name] =~ /pong/ {
          mutate { add_field => { "service_type" => "pong" } }
        } else if [kubernetes][container][name] =~ /notification/ {
          mutate { add_field => { "service_type" => "notifications" } }
        } else if [kubernetes][container][name] =~ /frontend/ {
          mutate { add_field => { "service_type" => "frontend" } }
        }
      }
      
      # Remove fields we don't need
      mutate {
        remove_field => [ "agent", "ecs", "input", "host" ]
      }
    }
    
    output {
      elasticsearch {
        hosts => ["https://eck-elasticsearch-es-http.default.svc.cluster.local:9200"]
        ssl => true
        ssl_certificate_verification => false
        user => "elastic"
        password => "${ELASTICSEARCH_PASSWORD}"
        index => "trascendence-logs-%{+YYYY.MM.dd}"
        template_name => "trascendence-logs"
        template_pattern => "trascendence-logs-*"
        template => {
          "index_patterns" => ["trascendence-logs-*"]
          "settings" => {
            "number_of_shards" => 1
            "number_of_replicas" => 0
            "refresh_interval" => "5s"
          }
          "mappings" => {
            "properties" => {
              "@timestamp" => { "type" => "date" }
              "log_level" => { "type" => "keyword" }
              "log_module" => { "type" => "keyword" }  
              "log_message" => { "type" => "text", "analyzer" => "standard" }
              "service_type" => { "type" => "keyword" }
              "service_name" => { "type" => "keyword" }
              "namespace" => { "type" => "keyword" }
              "pod_name" => { "type" => "keyword" }
              "log_path" => { "type" => "keyword" }
              "log_lineno" => { "type" => "integer" }
            }
          }
        }
      }
      
      # Debug output (remove in production)
      # stdout { codec => rubydebug }
    }
    
  django.conf: |
    input {
      # Direct input for Django structured logs
      http {
        port => 8080
        host => "0.0.0.0"
        codec => "json"
      }
    }
    
    filter {
      # Django logs are already structured, just add metadata
      mutate {
        add_field => { "log_source" => "django_direct" }
      }
      
      # Ensure timestamp is properly formatted
      if [time] {
        date {
          match => [ "time", "yyyy-MM-dd HH:mm:ss,SSS", "ISO8601" ]
          target => "@timestamp"
        }
      }
    }
    
    output {
      elasticsearch {
        hosts => ["https://eck-elasticsearch-es-http.default.svc.cluster.local:9200"]
        ssl => true
        ssl_certificate_verification => false
        user => "elastic" 
        password => "${ELASTICSEARCH_PASSWORD}"
        index => "django-direct-logs-%{+YYYY.MM.dd}"
      }
    }