# Clean & Simplified Observability Stack Configuration
# Elasticsearch + Kibana + Filebeat + Prometheus + Grafana

# Global Configuration
global:
  namespace:
    create: true
    name: "observability"  # Deploy observability stack to observability namespace
  tls:
    enabled: true
    ca:
      create: true
      secretName: "observability-ca-tls"
      name: "selfsigned-issuer"
      kind: ClusterIssuer

# Elasticsearch using ECK (Elastic Cloud on Kubernetes)
eck-elasticsearch:
  enabled: true
  fullnameOverride: "eck-elasticsearch"
  version: 8.6.0
  
  nodeSets:
  - name: default
    count: 1
    config:
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            requests:
              cpu: 200m
              memory: 1Gi
            limits:
              cpu: 1000m
              memory: 2Gi
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        storageClassName: local-path
        resources:
          requests:
            storage: 2Gi

# Kibana using ECK
eck-kibana:
  enabled: true
  fullnameOverride: "eck-kibana"
  version: 8.6.0
  
  # Labels that will be applied to Kibana
  labels: {}
  
  # Annotations that will be applied to Kibana
  annotations: {}
  
  spec:
    # Count of Kibana replicas to create
    count: 1
    
    # Reference to ECK-managed Elasticsearch resource
    elasticsearchRef:
      name: "eck-elasticsearch"
    
    # The Kibana configuration (kibana.yml)
    config:
      server.host: "0.0.0.0"
      server.port: 5601
      server.publicBaseUrl: "https://kibana.trascendence.local"
      
    # The HTTP layer configuration for Kibana
    http:
      tls:
        selfSignedCertificate:
          disabled: false
    
    # PodTemplate provides customisation options
    podTemplate:
      spec:
        containers:
        - name: kibana
          resources:
            requests:
              cpu: 100m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 1Gi
    
    # Number of revisions to retain
    revisionHistoryLimit: 2
    
    # Control Kibana Secure Settings
    secureSettings: []
    
    # Settings for configuring stack monitoring
    monitoring: {}
  
  # IMPORTANT: Completely disable ingress - we use IngressRoutes instead
  ingress:
    enabled: false

# IngressRoutes configuration (separate from traditional ingress)
ingressroutes:
  enabled: true
  domain: "trascendence.local"

# Logstash using ECK - for log processing and enrichment
eck-logstash:
  enabled: true
  fullnameOverride: "eck-logstash"
  version: 8.6.0
  
  # Single instance for resource efficiency
  count: 1
  
  # Reference to our Elasticsearch cluster  
  elasticsearchRefs:
    - name: "eck-elasticsearch"
      clusterName: "eck-elasticsearch"
  
  # Logstash configuration (logstash.yml equivalent)
  config:
    pipeline.workers: 1
    pipeline.batch.size: 125
    pipeline.batch.delay: 50
    
  # Reference the pipeline configuration from Secret  
  pipelinesRef:
    secretName: "logstash-pipelines-secret"
      
  # Pod template with resource limits for limited RAM
  podTemplate:
    spec:
      containers:
      - name: logstash
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
        env:
        - name: ELASTICSEARCH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: eck-elasticsearch-es-elastic-user
              key: elastic
        - name: LS_JAVA_OPTS
          value: "-Xmx512m -Xms512m"

# Filebeat for log collection
filebeat:
  enabled: false  # Disable the chart dependency, we use our own template
  # Custom filebeat configuration (for our template)
  customEnabled: true
  filebeatConfig:
    filebeat.yml: |
      filebeat.inputs:
      - type: container
        paths:
          - /var/log/containers/*.log
        processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"
        # Parse JSON logs from Django
        - decode_json_fields:
            fields: ["message"]
            target: "app"
            overwrite_keys: true
            add_error_key: true
        # Add service information
        - add_fields:
            target: service
            fields:
              name: '${kubernetes.container.name:unknown}'
              namespace: '${kubernetes.namespace:default}'
        
      # Send logs to Logstash for processing (instead of direct to Elasticsearch)
      output.logstash:
        hosts: ["eck-logstash-beats.observability.svc.cluster.local:5044"]
        
      # Enable monitoring
      http.enabled: true
      http.host: 0.0.0.0
      http.port: 5066

      logging.level: info
      logging.to_stderr: true
  
  # Environment variables
  extraEnvs:
    - name: ELASTICSEARCH_PASSWORD
      valueFrom:
        secretKeyRef:
          name: eck-elasticsearch-es-elastic-user
          key: elastic
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
  
  # Resources (enhanced for production)
  resources:
    requests:
      cpu: 100m
      memory: 100Mi
    limits:
      cpu: 500m
      memory: 500Mi
      
  # Volume mounts - using template-defined volumes
  # extraVolumes removed to prevent duplicates with template
        
  extraVolumeMounts: []
  # Run as DaemonSet
  daemonset:
    enabled: true
    
  # Security context
  securityContext:
    runAsUser: 0
    privileged: true
  
  # Health probes
  livenessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          curl --fail 127.0.0.1:5066
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5

  readinessProbe:
    exec:
      command:
        - sh
        - -c
        - |
          #!/usr/bin/env bash -e
          filebeat test output
    failureThreshold: 3
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
  
  # Service account management
  managedServiceAccount: true
  
  # Cluster role rules for accessing Kubernetes API
  clusterRoleRules:
    - apiGroups:
        - ""
      resources:
        - namespaces
        - nodes
        - pods
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - "apps"
      resources:
        - replicasets
      verbs:
        - get
        - list
        - watch

# Prometheus with Grafana integration
prometheus:
  enabled: true
  alertmanager:
    enabled: true
  grafana:
    enabled: false  # Use separate Grafana below
  
  prometheus:
    prometheusSpec:
      retention: 30d
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1000m
          memory: 2Gi
      
      # ServiceMonitor selector - enable Prometheus to discover ServiceMonitors
      serviceMonitorSelector: {}
      serviceMonitorSelectorNilUsesHelmValues: false
      
      # PodMonitor selector
      podMonitorSelector: {}
      podMonitorSelectorNilUsesHelmValues: false
      
      # Rule selector  
      ruleSelector: {}
      ruleSelectorNilUsesHelmValues: false

# Grafana for metrics and logs visualization
grafana:
  enabled: true
  adminUser: admin
  adminPassword: admin123
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 512Mi
  
  service:
    type: ClusterIP
    port: 80
  
  # Grafana configuration
  grafana.ini:
    server:
      root_url: "https://grafana.trascendence.local"
    security:
      admin_user: admin
      admin_password: admin123
  
  # Pre-configured datasources
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://my-umbrella-prometheus-prometheus:9090
        isDefault: true
        editable: true
      - name: Elasticsearch
        type: elasticsearch
        access: proxy
        url: https://eck-elasticsearch-es-http.elk.svc.cluster.local:9200
        isDefault: false
        editable: true
        basicAuth: true
        basicAuthUser: elastic
        secureJsonData:
          basicAuthPassword: "9xqNVM248GdsWQ14B4dh298j"
        jsonData:
          esVersion: "8.6.0"
          timeField: "@timestamp"
          logMessageField: message
          logLevelField: level
          includeFrozen: false
          xpack: true
  
  # Pre-configured dashboards  
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'k8s-dashboards'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/k8s
      - name: 'postgres-dashboards'
        orgId: 1
        folder: 'PostgreSQL'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/postgres
      - name: 'django-dashboards'
        orgId: 1
        folder: 'Django Services'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/django
      - name: 'redis-dashboards'
        orgId: 1
        folder: 'Redis'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/redis
      - name: 'trascendence-dashboards'
        orgId: 1
        folder: 'Trascendence'
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/trascendence
  
  # Dashboard ConfigMaps - Using gnetId for automatic download
  dashboards:
    k8s:
      k8s-cluster-overview:
        gnetId: 15282
        revision: 1
        datasource: Prometheus
      k8s-node-exporter:
        gnetId: 1860
        revision: 37
        datasource: Prometheus
    postgres:
      postgres-overview:
        gnetId: 9628
        revision: 7
        datasource: Prometheus
      postgres-database:
        gnetId: 455
        revision: 2
        datasource: Prometheus
    django:
      django-overview:
        gnetId: 17658
        revision: 1
        datasource: Prometheus
    redis:
      redis-overview:
        gnetId: 763
        revision: 5
        datasource: Prometheus
      redis-dashboard:
        gnetId: 11835
        revision: 1
        datasource: Prometheus

# Service Monitor Configuration for Prometheus
serviceMonitors:
  enabled: true  # Re-enable ServiceMonitors for Prometheus
  loginService:
    enabled: true
    port: 8000
    path: /metrics
    interval: 30s
  chatService:
    enabled: true
    port: 8001
    path: /metrics
    interval: 30s
  taskService:
    enabled: true
    port: 8002
    path: /metrics
    interval: 30s
  pongService:
    enabled: true
    port: 8004
    path: /metrics
    interval: 30s
  notificationService:
    enabled: true
    port: 8003
    path: /metrics
    interval: 30s